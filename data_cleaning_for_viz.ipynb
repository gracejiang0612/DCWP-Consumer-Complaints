{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Data Cleaning for Visualization: Eddie Cumart Ice Cream Truck Analysis\n",
        "\n",
        "**Preparing clean data for visualization focusing on 2020-present complaints**\n",
        "\n",
        "This notebook will:\n",
        "1. Filter data from 2020 to present\n",
        "2. Clean and standardize Eddie Cumart business names\n",
        "3. Rank top 5 most complained businesses\n",
        "4. Export to CSV for visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"üìä Data Cleaning for Visualization - Eddie Cumart Ice Cream Analysis\")\n",
        "print(\"=\" * 70)\n",
        "print(\"üîß Libraries loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Load the dataset\n",
        "print(\"üìã STEP 1: LOADING DATA\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "df = pd.read_csv('DCWP_Consumer_Complaints_20250623.csv')\n",
        "\n",
        "print(f\"‚úÖ Dataset loaded successfully!\")\n",
        "print(f\"‚Ä¢ Total complaints: {len(df):,}\")\n",
        "print(f\"‚Ä¢ Date range: {df['Intake Date'].min()} to {df['Intake Date'].max()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 2: Filter data from 2020 onwards\n",
        "print(\"\\nüìÖ STEP 2: FILTERING DATA (2020 - PRESENT)\")\n",
        "print(\"-\" * 45)\n",
        "\n",
        "# Convert date column to datetime\n",
        "df['Intake Date'] = pd.to_datetime(df['Intake Date'], errors='coerce')\n",
        "\n",
        "# Filter for 2020 onwards\n",
        "df_2020_plus = df[df['Intake Date'].dt.year >= 2020].copy()\n",
        "\n",
        "print(f\"‚úÖ Data filtered successfully!\")\n",
        "print(f\"‚Ä¢ Original complaints: {len(df):,}\")\n",
        "print(f\"‚Ä¢ 2020-present complaints: {len(df_2020_plus):,}\")\n",
        "print(f\"‚Ä¢ Filtered out: {len(df) - len(df_2020_plus):,} complaints\")\n",
        "print(f\"‚Ä¢ Date range after filtering: {df_2020_plus['Intake Date'].min()} to {df_2020_plus['Intake Date'].max()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 3: Clean and standardize Eddie Cumart business names\n",
        "print(\"\\nüç¶ STEP 3: CLEANING EDDIE CUMART BUSINESS NAMES\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Find all Eddie Cumart variations\n",
        "eddie_variations = df_2020_plus[df_2020_plus['Business Name'].str.contains('Eddie Cumart', case=False, na=False)]\n",
        "\n",
        "print(f\"Found Eddie Cumart business variations:\")\n",
        "eddie_name_counts = eddie_variations['Business Name'].value_counts()\n",
        "for name, count in eddie_name_counts.items():\n",
        "    print(f\"  ‚Ä¢ {name}: {count} complaints\")\n",
        "\n",
        "# Standardize all Eddie Cumart names\n",
        "STANDARD_EDDIE_NAME = \"NYC Soft Ice Cream Truck owned by Eddie Cumart\"\n",
        "\n",
        "# Create a copy for cleaning\n",
        "df_clean = df_2020_plus.copy()\n",
        "\n",
        "# Update all Eddie Cumart variations to standard name\n",
        "eddie_mask = df_clean['Business Name'].str.contains('Eddie Cumart', case=False, na=False)\n",
        "df_clean.loc[eddie_mask, 'Business Name'] = STANDARD_EDDIE_NAME\n",
        "\n",
        "print(f\"\\n‚úÖ Eddie Cumart names standardized!\")\n",
        "print(f\"‚Ä¢ Total Eddie Cumart complaints (2020+): {eddie_mask.sum()}\")\n",
        "print(f\"‚Ä¢ Standardized to: '{STANDARD_EDDIE_NAME}'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 4: Rank top 5 most complained businesses\n",
        "print(\"\\nüèÜ STEP 4: RANKING TOP 5 MOST COMPLAINED BUSINESSES\")\n",
        "print(\"-\" * 55)\n",
        "\n",
        "# Filter out rows with missing business names\n",
        "businesses_with_names = df_clean[df_clean['Business Name'].notna() & (df_clean['Business Name'] != '')]\n",
        "\n",
        "# Count complaints by business name\n",
        "business_complaints = businesses_with_names['Business Name'].value_counts()\n",
        "\n",
        "# Get top 5\n",
        "top_5_businesses = business_complaints.head(5)\n",
        "\n",
        "print(f\"ü•á TOP 5 MOST COMPLAINED BUSINESSES (2020-Present):\")\n",
        "print(\"-\" * 55)\n",
        "for rank, (business, count) in enumerate(top_5_businesses.items(), 1):\n",
        "    print(f\"{rank}. {business}\")\n",
        "    print(f\"   üìä {count:,} complaints\")\n",
        "    print()\n",
        "\n",
        "# Check if Eddie Cumart is in top 5\n",
        "if STANDARD_EDDIE_NAME in top_5_businesses.index:\n",
        "    eddie_rank = top_5_businesses.index.get_loc(STANDARD_EDDIE_NAME) + 1\n",
        "    eddie_complaints = top_5_businesses[STANDARD_EDDIE_NAME]\n",
        "    print(f\"üç¶ Eddie Cumart ranks #{eddie_rank} with {eddie_complaints:,} complaints!\")\n",
        "else:\n",
        "    eddie_complaints = business_complaints.get(STANDARD_EDDIE_NAME, 0)\n",
        "    eddie_rank = business_complaints.index.get_loc(STANDARD_EDDIE_NAME) + 1 if STANDARD_EDDIE_NAME in business_complaints.index else \"Not found\"\n",
        "    print(f\"üç¶ Eddie Cumart has {eddie_complaints:,} complaints (rank #{eddie_rank})\")\n",
        "\n",
        "print(f\"\\n‚úÖ Top 5 businesses identified successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 5: Create CSV file for visualization\n",
        "print(\"\\nüíæ STEP 5: CREATING CSV FILE FOR VISUALIZATION\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Create DataFrame for export\n",
        "viz_data = pd.DataFrame({\n",
        "    'Business_Name': top_5_businesses.index,\n",
        "    'Complaint_Numbers': top_5_businesses.values\n",
        "})\n",
        "\n",
        "# Add rank column\n",
        "viz_data['Rank'] = range(1, len(viz_data) + 1)\n",
        "\n",
        "# Reorder columns\n",
        "viz_data = viz_data[['Rank', 'Business_Name', 'Complaint_Numbers']]\n",
        "\n",
        "print(\"üìä Data prepared for visualization:\")\n",
        "print(viz_data.to_string(index=False))\n",
        "\n",
        "# Save to CSV\n",
        "csv_filename = 'top_5_businesses_complaints_2020_present.csv'\n",
        "viz_data.to_csv(csv_filename, index=False)\n",
        "\n",
        "print(f\"\\n‚úÖ CSV file created successfully!\")\n",
        "print(f\"‚Ä¢ Filename: {csv_filename}\")\n",
        "print(f\"‚Ä¢ Rows: {len(viz_data)}\")\n",
        "print(f\"‚Ä¢ Columns: {len(viz_data.columns)}\")\n",
        "print(f\"‚Ä¢ Ready for data visualization!\")\n",
        "\n",
        "# Display summary statistics\n",
        "print(f\"\\nüìà SUMMARY STATISTICS:\")\n",
        "print(f\"‚Ä¢ Total complaints for top 5: {viz_data['Complaint_Numbers'].sum():,}\")\n",
        "print(f\"‚Ä¢ Average complaints per business: {viz_data['Complaint_Numbers'].mean():.1f}\")\n",
        "print(f\"‚Ä¢ Highest complaint count: {viz_data['Complaint_Numbers'].max():,}\")\n",
        "print(f\"‚Ä¢ Lowest complaint count: {viz_data['Complaint_Numbers'].min():,}\")\n",
        "\n",
        "# Show the Eddie Cumart highlight\n",
        "if STANDARD_EDDIE_NAME in viz_data['Business_Name'].values:\n",
        "    print(f\"\\nüç¶ EDDIE CUMART HIGHLIGHT:\")\n",
        "    eddie_row = viz_data[viz_data['Business_Name'] == STANDARD_EDDIE_NAME]\n",
        "    print(f\"‚Ä¢ Rank: #{eddie_row['Rank'].iloc[0]}\")\n",
        "    print(f\"‚Ä¢ Complaints: {eddie_row['Complaint_Numbers'].iloc[0]:,}\")\n",
        "    print(f\"‚Ä¢ This will be the focus of your visualization story!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final Summary\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üéØ DATA CLEANING COMPLETED SUCCESSFULLY!\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(f\"\\n‚úÖ WHAT WE ACCOMPLISHED:\")\n",
        "print(f\"   1. üìÖ Filtered data from 2020 to present\")\n",
        "print(f\"   2. üç¶ Standardized Eddie Cumart business names\")\n",
        "print(f\"   3. üèÜ Identified top 5 most complained businesses\")\n",
        "print(f\"   4. üíæ Created clean CSV file for visualization\")\n",
        "\n",
        "print(f\"\\nüìä KEY NUMBERS:\")\n",
        "print(f\"   ‚Ä¢ Original dataset: {len(df):,} complaints\")\n",
        "print(f\"   ‚Ä¢ 2020-present data: {len(df_2020_plus):,} complaints\")\n",
        "print(f\"   ‚Ä¢ Eddie Cumart complaints: {eddie_mask.sum():,}\")\n",
        "print(f\"   ‚Ä¢ Top 5 businesses: {len(viz_data)} entries\")\n",
        "\n",
        "print(f\"\\nüìÅ OUTPUT FILE:\")\n",
        "print(f\"   ‚Ä¢ File: {csv_filename}\")\n",
        "print(f\"   ‚Ä¢ Columns: Rank, Business_Name, Complaint_Numbers\")\n",
        "print(f\"   ‚Ä¢ Ready for your visualization story!\")\n",
        "\n",
        "print(f\"\\nüöÄ NEXT STEPS:\")\n",
        "print(f\"   1. Use the CSV file for creating charts/graphs\")\n",
        "print(f\"   2. Focus on Eddie Cumart's ranking in your story\")\n",
        "print(f\"   3. Consider time-series analysis for trend visualization\")\n",
        "print(f\"   4. Add geographic or complaint-type breakdowns if needed\")\n",
        "\n",
        "print(f\"\\nüç¶ Your Eddie Cumart ice cream truck story is ready for visualization!\")\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Data Cleaning for Consumer Complaints Visualization\n",
        "\n",
        "**Comprehensive Data Cleaning Pipeline**\n",
        "\n",
        "This notebook systematically cleans the NYC consumer complaints dataset to prepare it for data visualization. We'll address:\n",
        "\n",
        "- ‚úÖ Business name standardization and deduplication\n",
        "- ‚úÖ Missing data handling\n",
        "- ‚úÖ Date formatting and validation\n",
        "- ‚úÖ Monetary amount cleaning\n",
        "- ‚úÖ Geographic data standardization\n",
        "- ‚úÖ Category normalization\n",
        "- ‚úÖ Outlier detection and handling\n",
        "- ‚úÖ Export clean dataset for visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import re\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configure pandas\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 20)\n",
        "pd.set_option('display.width', None)\n",
        "\n",
        "print(\"üì¶ Libraries imported successfully!\")\n",
        "print(\"üßπ Ready to clean the consumer complaints dataset for visualization\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "print(\"üìä LOADING AND INITIAL ASSESSMENT\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv('DCWP_Consumer_Complaints_20250623.csv')\n",
        "\n",
        "print(f\"‚úÖ Dataset loaded successfully!\")\n",
        "print(f\"üìè Shape: {df.shape[0]:,} rows √ó {df.shape[1]} columns\")\n",
        "print(f\"üíæ Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
        "\n",
        "# Initial data quality assessment\n",
        "print(f\"\\nüîç DATA QUALITY OVERVIEW:\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Missing data summary\n",
        "missing_summary = df.isnull().sum()\n",
        "missing_percent = (missing_summary / len(df)) * 100\n",
        "missing_df = pd.DataFrame({\n",
        "    'Missing_Count': missing_summary,\n",
        "    'Missing_Percent': missing_percent\n",
        "}).sort_values('Missing_Percent', ascending=False)\n",
        "\n",
        "print(f\"Columns with missing data: {(missing_df['Missing_Count'] > 0).sum()}\")\n",
        "print(f\"Most problematic columns:\")\n",
        "for col in missing_df.head(5).index:\n",
        "    if missing_df.loc[col, 'Missing_Count'] > 0:\n",
        "        print(f\"  ‚Ä¢ {col}: {missing_df.loc[col, 'Missing_Count']:,} missing ({missing_df.loc[col, 'Missing_Percent']:.1f}%)\")\n",
        "\n",
        "print(f\"\\nColumn data types:\")\n",
        "print(f\"  ‚Ä¢ Object: {(df.dtypes == 'object').sum()}\")\n",
        "print(f\"  ‚Ä¢ Numeric: {(df.dtypes.isin(['int64', 'float64'])).sum()}\")\n",
        "print(f\"  ‚Ä¢ Other: {(~df.dtypes.isin(['object', 'int64', 'float64'])).sum()}\")\n",
        "\n",
        "# Store original for comparison\n",
        "df_original = df.copy()\n",
        "print(f\"\\nüìã Original dataset backed up for comparison\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Clean Business Names\n",
        "print(\"üè¢ STEP 1: BUSINESS NAME STANDARDIZATION\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "def clean_business_names(df):\n",
        "    \"\"\"Standardize business names and merge similar variations\"\"\"\n",
        "    df_clean = df.copy()\n",
        "    \n",
        "    # Track changes\n",
        "    changes_made = 0\n",
        "    \n",
        "    # Clean basic formatting\n",
        "    df_clean['Business Name'] = df_clean['Business Name'].astype(str).str.strip()\n",
        "    \n",
        "    # Remove leading/trailing quotes and extra spaces\n",
        "    df_clean['Business Name'] = df_clean['Business Name'].str.replace(r'^[\"\\']|[\"\\']$', '', regex=True)\n",
        "    df_clean['Business Name'] = df_clean['Business Name'].str.replace(r'\\s+', ' ', regex=True)\n",
        "    \n",
        "    # Handle specific cases we know about\n",
        "    print(\"üîÑ Standardizing known business name variations...\")\n",
        "    \n",
        "    # Eddie Cumart case (from our previous analysis)\n",
        "    eddie_mask = df_clean['Business Name'].str.contains('Eddie Cumart', case=False, na=False)\n",
        "    eddie_count = eddie_mask.sum()\n",
        "    if eddie_count > 0:\n",
        "        df_clean.loc[eddie_mask, 'Business Name'] = 'NYC Soft Ice Cream Truck owned by Eddie Cumart'\n",
        "        changes_made += eddie_count\n",
        "        print(f\"  ‚Ä¢ Standardized {eddie_count} Eddie Cumart business variations\")\n",
        "    \n",
        "    # Generic cleaning patterns\n",
        "    cleaning_patterns = [\n",
        "        # Remove excessive punctuation\n",
        "        (r'[!@#$%^&*()_+=\\[\\]{}|;:,.<>?~`-]{2,}', ' '),\n",
        "        # Standardize common business suffixes\n",
        "        (r'\\b(inc|incorporated|corp|corporation|llc|ltd|limited)\\b\\.?', lambda x: x.group(1).upper(), re.IGNORECASE),\n",
        "        # Clean up \"unlicensed\" variations\n",
        "        (r'\\bunlicensed\\s+(.+)', r'Unlicensed \\1', re.IGNORECASE),\n",
        "    ]\n",
        "    \n",
        "    for pattern, replacement, *flags in cleaning_patterns:\n",
        "        flag = flags[0] if flags else 0\n",
        "        before_count = len(df_clean['Business Name'].unique())\n",
        "        df_clean['Business Name'] = df_clean['Business Name'].str.replace(pattern, replacement, regex=True, flags=flag)\n",
        "        after_count = len(df_clean['Business Name'].unique())\n",
        "        changes_made += before_count - after_count\n",
        "    \n",
        "    # Handle obvious unknowns and mark them clearly\n",
        "    unknown_patterns = [\n",
        "        r'^\\s*$',  # Empty strings\n",
        "        r'^nan$',  # String 'nan'\n",
        "        r'^(unknown|n/a|na|not available|not provided|none)$',  # Explicit unknowns\n",
        "        r'^.{1,3}$',  # Very short names\n",
        "    ]\n",
        "    \n",
        "    for pattern in unknown_patterns:\n",
        "        unknown_mask = df_clean['Business Name'].str.match(pattern, case=False, na=False)\n",
        "        unknown_count = unknown_mask.sum()\n",
        "        if unknown_count > 0:\n",
        "            df_clean.loc[unknown_mask, 'Business Name'] = '[UNKNOWN_BUSINESS]'\n",
        "            print(f\"  ‚Ä¢ Marked {unknown_count} businesses as unknown using pattern: {pattern}\")\n",
        "    \n",
        "    # Handle NaN values\n",
        "    nan_mask = df_clean['Business Name'].isna()\n",
        "    nan_count = nan_mask.sum()\n",
        "    if nan_count > 0:\n",
        "        df_clean.loc[nan_mask, 'Business Name'] = '[UNKNOWN_BUSINESS]'\n",
        "        print(f\"  ‚Ä¢ Marked {nan_count} NaN business names as unknown\")\n",
        "    \n",
        "    print(f\"‚úÖ Business name cleaning completed. Changes made: {changes_made}\")\n",
        "    return df_clean\n",
        "\n",
        "# Apply business name cleaning\n",
        "df_clean = clean_business_names(df)\n",
        "\n",
        "print(f\"\\nüìä Results:\")\n",
        "print(f\"‚Ä¢ Unique business names before: {df['Business Name'].nunique():,}\")\n",
        "print(f\"‚Ä¢ Unique business names after: {df_clean['Business Name'].nunique():,}\")\n",
        "print(f\"‚Ä¢ Unknown businesses: {(df_clean['Business Name'] == '[UNKNOWN_BUSINESS]').sum():,}\")\n",
        "\n",
        "# Show top business names after cleaning\n",
        "print(f\"\\nüèÜ Top 10 businesses after cleaning:\")\n",
        "top_businesses = df_clean['Business Name'].value_counts().head(10)\n",
        "for i, (business, count) in enumerate(top_businesses.items(), 1):\n",
        "    print(f\"{i:2d}. {business:<40} ({count:>4,} complaints)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 2: Clean Date Columns\n",
        "print(\"üìÖ STEP 2: DATE CLEANING AND VALIDATION\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "def clean_dates(df):\n",
        "    \"\"\"Clean and validate date columns\"\"\"\n",
        "    df_clean = df.copy()\n",
        "    \n",
        "    date_columns = ['Intake Date', 'Result Date']\n",
        "    \n",
        "    for col in date_columns:\n",
        "        if col in df_clean.columns:\n",
        "            print(f\"üîÑ Cleaning {col}...\")\n",
        "            \n",
        "            # Store original for comparison\n",
        "            original_valid = df_clean[col].notna().sum()\n",
        "            \n",
        "            # Convert to datetime\n",
        "            df_clean[col] = pd.to_datetime(df_clean[col], errors='coerce')\n",
        "            \n",
        "            # Check for invalid dates\n",
        "            new_valid = df_clean[col].notna().sum()\n",
        "            invalid_count = original_valid - new_valid\n",
        "            \n",
        "            if invalid_count > 0:\n",
        "                print(f\"  ‚ö†Ô∏è  {invalid_count} invalid dates found and converted to NaT\")\n",
        "            \n",
        "            # Check for unrealistic dates (future dates or too far in past)\n",
        "            current_date = pd.Timestamp.now()\n",
        "            future_dates = df_clean[col] > current_date\n",
        "            very_old_dates = df_clean[col] < pd.Timestamp('1900-01-01')\n",
        "            \n",
        "            if future_dates.sum() > 0:\n",
        "                print(f\"  ‚ö†Ô∏è  {future_dates.sum()} future dates found\")\n",
        "                df_clean.loc[future_dates, col] = pd.NaT\n",
        "            \n",
        "            if very_old_dates.sum() > 0:\n",
        "                print(f\"  ‚ö†Ô∏è  {very_old_dates.sum()} dates before 1900 found\")\n",
        "                df_clean.loc[very_old_dates, col] = pd.NaT\n",
        "            \n",
        "            # Summary statistics\n",
        "            if df_clean[col].notna().sum() > 0:\n",
        "                print(f\"  ‚úÖ Valid dates: {df_clean[col].notna().sum():,}\")\n",
        "                print(f\"  üìä Date range: {df_clean[col].min()} to {df_clean[col].max()}\")\n",
        "            else:\n",
        "                print(f\"  ‚ùå No valid dates found\")\n",
        "    \n",
        "    # Create additional date-based columns for visualization\n",
        "    print(f\"\\nüÜï Creating additional date columns for visualization...\")\n",
        "    \n",
        "    for col in date_columns:\n",
        "        if col in df_clean.columns and df_clean[col].notna().sum() > 0:\n",
        "            base_name = col.replace(' ', '_').lower()\n",
        "            \n",
        "            # Extract date components\n",
        "            df_clean[f'{base_name}_year'] = df_clean[col].dt.year\n",
        "            df_clean[f'{base_name}_month'] = df_clean[col].dt.month\n",
        "            df_clean[f'{base_name}_day_of_week'] = df_clean[col].dt.day_name()\n",
        "            df_clean[f'{base_name}_quarter'] = df_clean[col].dt.quarter\n",
        "            df_clean[f'{base_name}_month_name'] = df_clean[col].dt.month_name()\n",
        "            \n",
        "            print(f\"  ‚Ä¢ Created date components for {col}\")\n",
        "    \n",
        "    # Calculate processing time if both dates exist\n",
        "    if 'Intake Date' in df_clean.columns and 'Result Date' in df_clean.columns:\n",
        "        both_dates_exist = df_clean['Intake Date'].notna() & df_clean['Result Date'].notna()\n",
        "        if both_dates_exist.sum() > 0:\n",
        "            df_clean['processing_days'] = (df_clean['Result Date'] - df_clean['Intake Date']).dt.days\n",
        "            \n",
        "            # Remove negative processing times (likely data errors)\n",
        "            negative_processing = df_clean['processing_days'] < 0\n",
        "            if negative_processing.sum() > 0:\n",
        "                print(f\"  ‚ö†Ô∏è  {negative_processing.sum()} negative processing times found and set to NaN\")\n",
        "                df_clean.loc[negative_processing, 'processing_days'] = np.nan\n",
        "            \n",
        "            print(f\"  ‚úÖ Processing time calculated for {df_clean['processing_days'].notna().sum():,} complaints\")\n",
        "    \n",
        "    return df_clean\n",
        "\n",
        "# Apply date cleaning\n",
        "df_clean = clean_dates(df_clean)\n",
        "\n",
        "print(f\"\\nüìä Date cleaning summary:\")\n",
        "print(f\"‚Ä¢ Intake Date valid: {df_clean['Intake Date'].notna().sum():,} ({(df_clean['Intake Date'].notna().sum() / len(df_clean)) * 100:.1f}%)\")\n",
        "print(f\"‚Ä¢ Result Date valid: {df_clean['Result Date'].notna().sum():,} ({(df_clean['Result Date'].notna().sum() / len(df_clean)) * 100:.1f}%)\")\n",
        "if 'processing_days' in df_clean.columns:\n",
        "    print(f\"‚Ä¢ Processing time available: {df_clean['processing_days'].notna().sum():,} ({(df_clean['processing_days'].notna().sum() / len(df_clean)) * 100:.1f}%)\")\n",
        "    if df_clean['processing_days'].notna().sum() > 0:\n",
        "        print(f\"‚Ä¢ Average processing time: {df_clean['processing_days'].mean():.1f} days\")\n",
        "\n",
        "print(f\"\\nüÜï New columns created: {[col for col in df_clean.columns if col not in df.columns]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 3: Clean Monetary Amounts\n",
        "print(\"üí∞ STEP 3: MONETARY AMOUNT CLEANING\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "def clean_monetary_amounts(df):\n",
        "    \"\"\"Clean and validate monetary amount columns\"\"\"\n",
        "    df_clean = df.copy()\n",
        "    \n",
        "    monetary_columns = ['Refund Amount', 'Contract Cancelled Amount']\n",
        "    \n",
        "    for col in monetary_columns:\n",
        "        if col in df_clean.columns:\n",
        "            print(f\"üîÑ Cleaning {col}...\")\n",
        "            \n",
        "            # Store original for comparison\n",
        "            original_data = df_clean[col].copy()\n",
        "            original_non_null = original_data.notna().sum()\n",
        "            \n",
        "            # Convert to string first to handle mixed types\n",
        "            df_clean[col] = df_clean[col].astype(str)\n",
        "            \n",
        "            # Clean monetary formatting\n",
        "            # Remove currency symbols, commas, and extra spaces\n",
        "            df_clean[col] = df_clean[col].str.replace(r'[\\$,\\s]', '', regex=True)\n",
        "            \n",
        "            # Handle common variations\n",
        "            df_clean[col] = df_clean[col].replace({\n",
        "                'nan': np.nan,\n",
        "                'NaN': np.nan,\n",
        "                '': np.nan,\n",
        "                'None': np.nan,\n",
        "                'N/A': np.nan,\n",
        "                'n/a': np.nan,\n",
        "                '-': np.nan,\n",
        "                '0.00': np.nan,\n",
        "                '0': np.nan\n",
        "            })\n",
        "            \n",
        "            # Convert to numeric\n",
        "            df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')\n",
        "            \n",
        "            # Handle negative values (shouldn't exist for refunds/cancellations)\n",
        "            negative_mask = df_clean[col] < 0\n",
        "            if negative_mask.sum() > 0:\n",
        "                print(f\"  ‚ö†Ô∏è  {negative_mask.sum()} negative values found and set to NaN\")\n",
        "                df_clean.loc[negative_mask, col] = np.nan\n",
        "            \n",
        "            # Handle extremely large values (likely data errors)\n",
        "            # Set threshold at $1,000,000 for individual complaints\n",
        "            extreme_mask = df_clean[col] > 1000000\n",
        "            if extreme_mask.sum() > 0:\n",
        "                print(f\"  ‚ö†Ô∏è  {extreme_mask.sum()} extremely large values (>$1M) found\")\n",
        "                print(f\"      Max value: ${df_clean[col].max():,.2f}\")\n",
        "                # Don't remove automatically, but flag for review\n",
        "            \n",
        "            # Summary statistics\n",
        "            valid_amounts = df_clean[col].notna().sum()\n",
        "            if valid_amounts > 0:\n",
        "                print(f\"  ‚úÖ Valid amounts: {valid_amounts:,} ({(valid_amounts / len(df_clean)) * 100:.1f}%)\")\n",
        "                print(f\"  üìä Amount range: ${df_clean[col].min():,.2f} to ${df_clean[col].max():,.2f}\")\n",
        "                print(f\"  üìä Total amount: ${df_clean[col].sum():,.2f}\")\n",
        "                print(f\"  üìä Average amount: ${df_clean[col].mean():.2f}\")\n",
        "            else:\n",
        "                print(f\"  ‚ùå No valid amounts found\")\n",
        "    \n",
        "    # Create combined financial impact column\n",
        "    refund_col = 'Refund Amount'\n",
        "    cancel_col = 'Contract Cancelled Amount'\n",
        "    \n",
        "    if refund_col in df_clean.columns and cancel_col in df_clean.columns:\n",
        "        df_clean['total_financial_impact'] = (\n",
        "            df_clean[refund_col].fillna(0) + df_clean[cancel_col].fillna(0)\n",
        "        )\n",
        "        # Set to NaN if both original values were NaN\n",
        "        both_nan = df_clean[refund_col].isna() & df_clean[cancel_col].isna()\n",
        "        df_clean.loc[both_nan, 'total_financial_impact'] = np.nan\n",
        "        \n",
        "        print(f\"\\nüÜï Created total_financial_impact column\")\n",
        "        if df_clean['total_financial_impact'].notna().sum() > 0:\n",
        "            print(f\"  ‚Ä¢ Cases with financial impact: {df_clean['total_financial_impact'].notna().sum():,}\")\n",
        "            print(f\"  ‚Ä¢ Total financial impact: ${df_clean['total_financial_impact'].sum():,.2f}\")\n",
        "    \n",
        "    # Create categorical columns for visualization\n",
        "    for col in monetary_columns:\n",
        "        if col in df_clean.columns and df_clean[col].notna().sum() > 0:\n",
        "            cat_col = col.replace(' ', '_').lower() + '_category'\n",
        "            \n",
        "            # Create amount categories\n",
        "            df_clean[cat_col] = pd.cut(\n",
        "                df_clean[col], \n",
        "                bins=[0, 100, 500, 1000, 5000, float('inf')],\n",
        "                labels=['$0-100', '$100-500', '$500-1K', '$1K-5K', '$5K+'],\n",
        "                include_lowest=True\n",
        "            )\n",
        "            \n",
        "            print(f\"  ‚Ä¢ Created {cat_col} for visualization\")\n",
        "    \n",
        "    return df_clean\n",
        "\n",
        "# Apply monetary cleaning\n",
        "df_clean = clean_monetary_amounts(df_clean)\n",
        "\n",
        "print(f\"\\nüìä Monetary cleaning summary:\")\n",
        "for col in ['Refund Amount', 'Contract Cancelled Amount']:\n",
        "    if col in df_clean.columns:\n",
        "        valid_count = df_clean[col].notna().sum()\n",
        "        if valid_count > 0:\n",
        "            print(f\"‚Ä¢ {col}: {valid_count:,} valid amounts (${df_clean[col].sum():,.2f} total)\")\n",
        "        else:\n",
        "            print(f\"‚Ä¢ {col}: No valid amounts\")\n",
        "\n",
        "if 'total_financial_impact' in df_clean.columns:\n",
        "    impact_count = df_clean['total_financial_impact'].notna().sum()\n",
        "    if impact_count > 0:\n",
        "        print(f\"‚Ä¢ Total Financial Impact: {impact_count:,} cases (${df_clean['total_financial_impact'].sum():,.2f} total)\")\n",
        "\n",
        "print(f\"\\nüÜï New columns: {[col for col in df_clean.columns if col not in df.columns and col.endswith('_category')]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 4: Clean Geographic Data\n",
        "print(\"üó∫Ô∏è STEP 4: GEOGRAPHIC DATA CLEANING\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "def clean_geographic_data(df):\n",
        "    \"\"\"Clean and standardize geographic information\"\"\"\n",
        "    df_clean = df.copy()\n",
        "    \n",
        "    # Clean Borough names\n",
        "    print(\"üîÑ Cleaning Borough names...\")\n",
        "    if 'Borough' in df_clean.columns:\n",
        "        df_clean['Borough'] = df_clean['Borough'].str.strip()\n",
        "        df_clean['Borough'] = df_clean['Borough'].str.title()\n",
        "        \n",
        "        # Standardize common variations\n",
        "        borough_mapping = {\n",
        "            'New York': 'Manhattan',\n",
        "            'New York County': 'Manhattan',\n",
        "            'Kings': 'Brooklyn',\n",
        "            'Kings County': 'Brooklyn',\n",
        "            'Richmond': 'Staten Island',\n",
        "            'Richmond County': 'Staten Island',\n",
        "            'Queens County': 'Queens',\n",
        "            'Bronx County': 'Bronx',\n",
        "            'The Bronx': 'Bronx'\n",
        "        }\n",
        "        \n",
        "        for old_name, new_name in borough_mapping.items():\n",
        "            mask = df_clean['Borough'].str.contains(old_name, case=False, na=False)\n",
        "            if mask.sum() > 0:\n",
        "                df_clean.loc[mask, 'Borough'] = new_name\n",
        "                print(f\"  ‚Ä¢ Standardized {mask.sum()} '{old_name}' to '{new_name}'\")\n",
        "    \n",
        "    # Clean ZIP codes\n",
        "    print(\"üîÑ Cleaning ZIP codes...\")\n",
        "    if 'Postcode' in df_clean.columns:\n",
        "        # Convert to string and clean\n",
        "        df_clean['Postcode'] = df_clean['Postcode'].astype(str).str.strip()\n",
        "        \n",
        "        # Remove non-numeric characters (except in ZIP+4 format)\n",
        "        df_clean['Postcode'] = df_clean['Postcode'].str.replace(r'[^\\d-]', '', regex=True)\n",
        "        \n",
        "        # Handle common issues\n",
        "        df_clean['Postcode'] = df_clean['Postcode'].replace({\n",
        "            'nan': np.nan,\n",
        "            '': np.nan,\n",
        "            '0': np.nan,\n",
        "            '00000': np.nan\n",
        "        })\n",
        "        \n",
        "        # Validate ZIP code format (5 digits or 5-4 format)\n",
        "        valid_zip_pattern = r'^\\d{5}(-\\d{4})?$'\n",
        "        invalid_zips = df_clean['Postcode'].notna() & ~df_clean['Postcode'].str.match(valid_zip_pattern)\n",
        "        \n",
        "        if invalid_zips.sum() > 0:\n",
        "            print(f\"  ‚ö†Ô∏è  {invalid_zips.sum()} invalid ZIP codes found\")\n",
        "            df_clean.loc[invalid_zips, 'Postcode'] = np.nan\n",
        "        \n",
        "        # Extract 5-digit ZIP for analysis\n",
        "        df_clean['zip_5digit'] = df_clean['Postcode'].str[:5]\n",
        "        \n",
        "        print(f\"  ‚úÖ Valid ZIP codes: {df_clean['Postcode'].notna().sum():,}\")\n",
        "    \n",
        "    # Clean street addresses\n",
        "    print(\"üîÑ Cleaning street addresses...\")\n",
        "    address_columns = ['Street1', 'Street2', 'Street3']\n",
        "    \n",
        "    for col in address_columns:\n",
        "        if col in df_clean.columns:\n",
        "            df_clean[col] = df_clean[col].astype(str).str.strip()\n",
        "            df_clean[col] = df_clean[col].str.title()\n",
        "            \n",
        "            # Replace 'nan' strings with actual NaN\n",
        "            df_clean[col] = df_clean[col].replace('Nan', np.nan)\n",
        "    \n",
        "    # Create full address column\n",
        "    address_parts = []\n",
        "    for col in ['Building Nbr', 'Street1', 'Street2', 'Street3']:\n",
        "        if col in df_clean.columns:\n",
        "            address_parts.append(df_clean[col].astype(str))\n",
        "    \n",
        "    if address_parts:\n",
        "        df_clean['full_address'] = address_parts[0]\n",
        "        for part in address_parts[1:]:\n",
        "            df_clean['full_address'] += ' ' + part\n",
        "        \n",
        "        # Clean up the full address\n",
        "        df_clean['full_address'] = df_clean['full_address'].str.replace(r'\\s+', ' ', regex=True)\n",
        "        df_clean['full_address'] = df_clean['full_address'].str.replace(r'\\bnan\\b', '', regex=True, case=False)\n",
        "        df_clean['full_address'] = df_clean['full_address'].str.strip()\n",
        "        \n",
        "        print(f\"  ‚úÖ Created full_address column\")\n",
        "    \n",
        "    # Clean coordinates\n",
        "    print(\"üîÑ Cleaning coordinates...\")\n",
        "    coordinate_columns = ['Latitude', 'Longitude']\n",
        "    \n",
        "    for col in coordinate_columns:\n",
        "        if col in df_clean.columns:\n",
        "            df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')\n",
        "            \n",
        "            # Validate coordinate ranges\n",
        "            if col == 'Latitude':\n",
        "                # NYC latitude range approximately 40.4 to 40.9\n",
        "                invalid_lat = (df_clean[col] < 40.0) | (df_clean[col] > 41.0)\n",
        "                if invalid_lat.sum() > 0:\n",
        "                    print(f\"  ‚ö†Ô∏è  {invalid_lat.sum()} invalid latitudes found\")\n",
        "                    df_clean.loc[invalid_lat, col] = np.nan\n",
        "            \n",
        "            elif col == 'Longitude':\n",
        "                # NYC longitude range approximately -74.3 to -73.7\n",
        "                invalid_lon = (df_clean[col] < -75.0) | (df_clean[col] > -73.0)\n",
        "                if invalid_lon.sum() > 0:\n",
        "                    print(f\"  ‚ö†Ô∏è  {invalid_lon.sum()} invalid longitudes found\")\n",
        "                    df_clean.loc[invalid_lon, col] = np.nan\n",
        "    \n",
        "    # Create coordinate availability flag\n",
        "    df_clean['has_coordinates'] = (\n",
        "        df_clean['Latitude'].notna() & df_clean['Longitude'].notna()\n",
        "    )\n",
        "    \n",
        "    # Clean City names\n",
        "    print(\"üîÑ Cleaning City names...\")\n",
        "    if 'City' in df_clean.columns:\n",
        "        df_clean['City'] = df_clean['City'].str.strip().str.title()\n",
        "        \n",
        "        # Common NYC city name variations\n",
        "        city_mapping = {\n",
        "            'New York City': 'New York',\n",
        "            'Nyc': 'New York',\n",
        "            'Manhattan': 'New York',\n",
        "            'Brooklyn': 'Brooklyn',\n",
        "            'Bronx': 'Bronx',\n",
        "            'Queens': 'Queens',\n",
        "            'Staten Island': 'Staten Island'\n",
        "        }\n",
        "        \n",
        "        for old_city, new_city in city_mapping.items():\n",
        "            mask = df_clean['City'].str.contains(old_city, case=False, na=False)\n",
        "            if mask.sum() > 0:\n",
        "                df_clean.loc[mask, 'City'] = new_city\n",
        "    \n",
        "    return df_clean\n",
        "\n",
        "# Apply geographic cleaning\n",
        "df_clean = clean_geographic_data(df_clean)\n",
        "\n",
        "print(f\"\\nüìä Geographic cleaning summary:\")\n",
        "if 'Borough' in df_clean.columns:\n",
        "    print(f\"‚Ä¢ Borough distribution:\")\n",
        "    borough_counts = df_clean['Borough'].value_counts()\n",
        "    for borough, count in borough_counts.items():\n",
        "        print(f\"  - {borough}: {count:,} complaints\")\n",
        "\n",
        "if 'Postcode' in df_clean.columns:\n",
        "    print(f\"‚Ä¢ ZIP codes: {df_clean['Postcode'].notna().sum():,} valid ({(df_clean['Postcode'].notna().sum() / len(df_clean)) * 100:.1f}%)\")\n",
        "\n",
        "if 'has_coordinates' in df_clean.columns:\n",
        "    print(f\"‚Ä¢ Coordinates: {df_clean['has_coordinates'].sum():,} complete pairs ({(df_clean['has_coordinates'].sum() / len(df_clean)) * 100:.1f}%)\")\n",
        "\n",
        "print(f\"\\nüÜï New geographic columns: {[col for col in df_clean.columns if col in ['full_address', 'zip_5digit', 'has_coordinates']]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 5: Standardize Categories and Complaint Codes\n",
        "print(\"üìÇ STEP 5: CATEGORY AND COMPLAINT CODE STANDARDIZATION\")\n",
        "print(\"=\" * 65)\n",
        "\n",
        "def standardize_categories(df):\n",
        "    \"\"\"Standardize business categories and complaint codes\"\"\"\n",
        "    df_clean = df.copy()\n",
        "    \n",
        "    # Clean Business Categories\n",
        "    print(\"üîÑ Cleaning Business Categories...\")\n",
        "    if 'Business Category' in df_clean.columns:\n",
        "        df_clean['Business Category'] = df_clean['Business Category'].str.strip()\n",
        "        df_clean['Business Category'] = df_clean['Business Category'].str.title()\n",
        "        \n",
        "        # Standardize common variations\n",
        "        category_mapping = {\n",
        "            'Restaurant': 'Restaurant',\n",
        "            'Restaurants': 'Restaurant',\n",
        "            'Food Service': 'Restaurant',\n",
        "            'Grocery': 'Grocery-Retail',\n",
        "            'Grocery Store': 'Grocery-Retail',\n",
        "            'Retail': 'Grocery-Retail',\n",
        "            'Car Service': 'Car Service/Taxi',\n",
        "            'Taxi': 'Car Service/Taxi',\n",
        "            'Automotive': 'Automotive Service',\n",
        "            'Auto': 'Automotive Service',\n",
        "            'Electronics': 'Electronics Store',\n",
        "            'Electronic': 'Electronics Store',\n",
        "            'Laundry': 'Laundry/Dry Cleaning',\n",
        "            'Dry Cleaning': 'Laundry/Dry Cleaning',\n",
        "            'Beauty': 'Salons And Barbershop',\n",
        "            'Salon': 'Salons And Barbershop',\n",
        "            'Barbershop': 'Salons And Barbershop',\n",
        "        }\n",
        "        \n",
        "        for old_cat, new_cat in category_mapping.items():\n",
        "            mask = df_clean['Business Category'].str.contains(old_cat, case=False, na=False)\n",
        "            if mask.sum() > 0:\n",
        "                df_clean.loc[mask, 'Business Category'] = new_cat\n",
        "                print(f\"  ‚Ä¢ Standardized {mask.sum()} '{old_cat}' to '{new_cat}'\")\n",
        "    \n",
        "    # Clean Complaint Codes\n",
        "    print(\"üîÑ Cleaning Complaint Codes...\")\n",
        "    if 'Complaint Code' in df_clean.columns:\n",
        "        df_clean['Complaint Code'] = df_clean['Complaint Code'].str.strip()\n",
        "        \n",
        "        # Create simplified complaint categories for visualization\n",
        "        complaint_categories = {\n",
        "            'Pricing Issues': ['Overcharge', 'Price Gouging', 'Price Not Posted', 'Required Signage Not Posted'],\n",
        "            'Service Issues': ['Poor Service', 'Refusal of Service', 'Dissatisfaction with Provider'],\n",
        "            'Licensing Issues': ['Unlicensed', 'License', 'Permit'],\n",
        "            'Advertising Issues': ['Advertising', 'Misleading', 'False Advertising'],\n",
        "            'Delivery Issues': ['Non-Delivery', 'Delivery', 'Late Delivery'],\n",
        "            'Quality Issues': ['Defective', 'Poor Quality', 'Unsatisfactory'],\n",
        "            'Safety Issues': ['Health', 'Safety', 'Sanitation'],\n",
        "            'Other Issues': ['Other', 'Miscellaneous', 'General']\n",
        "        }\n",
        "        \n",
        "        df_clean['complaint_category'] = 'Other'\n",
        "        \n",
        "        for category, keywords in complaint_categories.items():\n",
        "            for keyword in keywords:\n",
        "                mask = df_clean['Complaint Code'].str.contains(keyword, case=False, na=False)\n",
        "                df_clean.loc[mask, 'complaint_category'] = category\n",
        "        \n",
        "        print(f\"  ‚úÖ Created complaint_category column\")\n",
        "    \n",
        "    # Clean Result/Outcome codes\n",
        "    print(\"üîÑ Cleaning Result codes...\")\n",
        "    if 'Result' in df_clean.columns:\n",
        "        df_clean['Result'] = df_clean['Result'].str.strip()\n",
        "        \n",
        "        # Create simplified outcome categories\n",
        "        outcome_mapping = {\n",
        "            'Resolved': ['Complaint Review Complete', 'Resolved', 'Closed', 'Satisfied'],\n",
        "            'Referred': ['Referred', 'Forwarded', 'Transferred'],\n",
        "            'Insufficient Info': ['Insufficient Info', 'Insufficient Information', 'Info Required'],\n",
        "            'No Action': ['No Action Required', 'No Violation', 'Unfounded'],\n",
        "            'Pending': ['Pending', 'In Progress', 'Under Review'],\n",
        "            'Other': ['Other', 'Miscellaneous']\n",
        "        }\n",
        "        \n",
        "        df_clean['outcome_category'] = 'Other'\n",
        "        \n",
        "        for outcome, keywords in outcome_mapping.items():\n",
        "            for keyword in keywords:\n",
        "                mask = df_clean['Result'].str.contains(keyword, case=False, na=False)\n",
        "                df_clean.loc[mask, 'outcome_category'] = outcome\n",
        "        \n",
        "        print(f\"  ‚úÖ Created outcome_category column\")\n",
        "    \n",
        "    # Clean Intake Channel\n",
        "    print(\"üîÑ Cleaning Intake Channels...\")\n",
        "    if 'Intake Channel' in df_clean.columns:\n",
        "        df_clean['Intake Channel'] = df_clean['Intake Channel'].str.strip()\n",
        "        \n",
        "        # Group similar channels\n",
        "        channel_mapping = {\n",
        "            'Phone': ['311', 'Phone', 'Call', 'Telephone'],\n",
        "            'Online': ['Online', 'Web', 'Website', 'Internet'],\n",
        "            'Email': ['Email', 'E-mail', 'Electronic Mail'],\n",
        "            'In Person': ['Walk-in', 'In Person', 'Office', 'Counter'],\n",
        "            'Mail': ['Mail', 'Letter', 'Postal', 'Hardcopy'],\n",
        "            'Other': ['Other', 'Miscellaneous']\n",
        "        }\n",
        "        \n",
        "        df_clean['channel_category'] = 'Other'\n",
        "        \n",
        "        for channel, keywords in channel_mapping.items():\n",
        "            for keyword in keywords:\n",
        "                mask = df_clean['Intake Channel'].str.contains(keyword, case=False, na=False)\n",
        "                df_clean.loc[mask, 'channel_category'] = channel\n",
        "        \n",
        "        print(f\"  ‚úÖ Created channel_category column\")\n",
        "    \n",
        "    # Create high-level business type categories\n",
        "    print(\"üîÑ Creating business type categories...\")\n",
        "    if 'Business Category' in df_clean.columns:\n",
        "        business_type_mapping = {\n",
        "            'Food & Beverage': ['Restaurant', 'Grocery-Retail', 'Food', 'Deli', 'Bar', 'Cafe'],\n",
        "            'Retail': ['Retail', 'Store', 'Shop', 'Electronics', 'Clothing', 'Jewelry'],\n",
        "            'Services': ['Service', 'Repair', 'Cleaning', 'Laundry', 'Salons', 'Beauty'],\n",
        "            'Transportation': ['Car Service', 'Taxi', 'Transportation', 'Parking', 'Tow'],\n",
        "            'Other': ['Other', 'Miscellaneous']\n",
        "        }\n",
        "        \n",
        "        df_clean['business_type'] = 'Other'\n",
        "        \n",
        "        for biz_type, keywords in business_type_mapping.items():\n",
        "            for keyword in keywords:\n",
        "                mask = df_clean['Business Category'].str.contains(keyword, case=False, na=False)\n",
        "                df_clean.loc[mask, 'business_type'] = biz_type\n",
        "        \n",
        "        print(f\"  ‚úÖ Created business_type column\")\n",
        "    \n",
        "    return df_clean\n",
        "\n",
        "# Apply category standardization\n",
        "df_clean = standardize_categories(df_clean)\n",
        "\n",
        "print(f\"\\nüìä Category standardization summary:\")\n",
        "\n",
        "# Show category distributions\n",
        "if 'Business Category' in df_clean.columns:\n",
        "    print(f\"‚Ä¢ Business Categories: {df_clean['Business Category'].nunique()} unique categories\")\n",
        "    top_categories = df_clean['Business Category'].value_counts().head(5)\n",
        "    for cat, count in top_categories.items():\n",
        "        print(f\"  - {cat}: {count:,} complaints\")\n",
        "\n",
        "if 'complaint_category' in df_clean.columns:\n",
        "    print(f\"‚Ä¢ Complaint Categories: {df_clean['complaint_category'].nunique()} simplified categories\")\n",
        "    complaint_dist = df_clean['complaint_category'].value_counts()\n",
        "    for cat, count in complaint_dist.items():\n",
        "        print(f\"  - {cat}: {count:,} complaints\")\n",
        "\n",
        "if 'outcome_category' in df_clean.columns:\n",
        "    print(f\"‚Ä¢ Outcome Categories: {df_clean['outcome_category'].nunique()} simplified outcomes\")\n",
        "    outcome_dist = df_clean['outcome_category'].value_counts()\n",
        "    for cat, count in outcome_dist.items():\n",
        "        print(f\"  - {cat}: {count:,} complaints\")\n",
        "\n",
        "if 'channel_category' in df_clean.columns:\n",
        "    print(f\"‚Ä¢ Channel Categories: {df_clean['channel_category'].nunique()} simplified channels\")\n",
        "    channel_dist = df_clean['channel_category'].value_counts()\n",
        "    for cat, count in channel_dist.items():\n",
        "        print(f\"  - {cat}: {count:,} complaints\")\n",
        "\n",
        "new_category_cols = [col for col in df_clean.columns if col.endswith('_category') or col == 'business_type']\n",
        "print(f\"\\nüÜï New category columns: {new_category_cols}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
